{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9a6edc",
   "metadata": {},
   "source": [
    "# üåäüèúÔ∏è Natural Disaster Prediction Model Training\n",
    "\n",
    "## Big Data and Deep Learning-Based Natural Disaster Prediction Using Multi-Source Environmental Data\n",
    "\n",
    "This notebook trains a multi-encoder deep learning model for predicting floods and droughts.\n",
    "\n",
    "### Architecture Overview:\n",
    "- **CNN Encoder**: Processes satellite features (NDVI, EVI, LST) - spatial patterns\n",
    "- **LSTM Encoder**: Processes weather sequences (temp, precipitation, wind, etc.) - temporal patterns\n",
    "- **MLP Encoder**: Processes static features (elevation, landcover, coordinates) - geographic context\n",
    "- **Mid-Level Fusion**: Combines all encoder outputs\n",
    "- **Prediction Heads**: Flood (binary) and Drought (multi-class) classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3725402",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eebd247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/leonnn/Desktop/DL for disaster\n",
      "Current working directory: /Users/leonnn/Desktop/DL for disaster\n",
      "PyTorch version: 2.10.0\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path - FIX for notebook\n",
    "if 'notebooks' in os.getcwd():\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "# Ensure project root is in path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Change to project root for consistent paths\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if hasattr(torch.backends, 'mps'):\n",
    "    print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003d91c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import project modules\n",
    "from configs.config import (\n",
    "    get_config, DATA_PATH, MODEL_DIR, LOG_DIR,\n",
    "    SATELLITE_FEATURES, WEATHER_FEATURES, STATIC_FEATURES\n",
    ")\n",
    "from src.dataset import (\n",
    "    DisasterDataProcessor, DisasterDataset, \n",
    "    create_dataloaders, compute_class_weights\n",
    ")\n",
    "from src.models import (\n",
    "    DisasterPredictionModel, MultiTaskLoss, create_model\n",
    ")\n",
    "from src.utils import (\n",
    "    setup_logger, MetricsCalculator, EarlyStopping,\n",
    "    CheckpointManager, TrainingHistory, set_seed, get_device,\n",
    "    format_time, plot_confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aefc20",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb56d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Using Apple Silicon MPS\n",
      "============================================================\n",
      "CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä Data Configuration:\n",
      "   Train/Val/Test: 0.7/0.15/0.15\n",
      "   Sequence length: 7 days\n",
      "   Grid size: 5x5\n",
      "   Batch size: 256\n",
      "\n",
      "üèóÔ∏è Model Configuration:\n",
      "   Encoder output dim: 128\n",
      "   CNN channels: [32, 64, 128]\n",
      "   LSTM hidden: 128, layers: 2\n",
      "   MLP hidden: [64, 128]\n",
      "\n",
      "üéØ Training Configuration:\n",
      "   Epochs: 100\n",
      "   Learning rate: 0.001\n",
      "   Early stopping patience: 10\n",
      "   Device: mps\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = get_config()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(config.data.random_seed)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "config.device = str(device)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Data Configuration:\")\n",
    "print(f\"   Train/Val/Test: {config.data.train_ratio}/{config.data.valid_ratio}/{config.data.test_ratio}\")\n",
    "print(f\"   Sequence length: {config.data.sequence_length} days\")\n",
    "print(f\"   Grid size: {config.data.grid_size}x{config.data.grid_size}\")\n",
    "print(f\"   Batch size: {config.data.batch_size}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Model Configuration:\")\n",
    "print(f\"   Encoder output dim: {config.model.encoder_output_dim}\")\n",
    "print(f\"   CNN channels: {config.model.cnn_channels}\")\n",
    "print(f\"   LSTM hidden: {config.model.lstm_hidden_size}, layers: {config.model.lstm_num_layers}\")\n",
    "print(f\"   MLP hidden: {config.model.mlp_hidden_sizes}\")\n",
    "\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"   Epochs: {config.training.num_epochs}\")\n",
    "print(f\"   Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"   Early stopping patience: {config.training.early_stopping_patience}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b44a4",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bc4a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loading data from /Users/leonnn/Desktop/DL for disaster/SEA_2024_FINAL_CLEAN.csv...\n",
      "Loaded 1,323,822 rows with 21 columns\n",
      "\n",
      "Data splits:\n",
      "  Train: 926,346 samples (70.0%)\n",
      "  Valid: 198,738 samples (15.0%)\n",
      "  Test:  198,738 samples (15.0%)\n",
      "Filled 2,227 missing values in 'ndvi' with 0.6749\n",
      "Filled 2,227 missing values in 'evi' with 0.4121\n",
      "Filled 544,641 missing values in 'lst' with 27.3900\n",
      "Filled 16,836 missing values in 'precip_mm' with 2.1642\n",
      "Filled 16,836 missing values in 'temp_c' with 25.0372\n",
      "Filled 16,836 missing values in 'dewpoint_c' with 20.9961\n",
      "Filled 16,836 missing values in 'wind_u' with -0.1192\n",
      "Filled 16,836 missing values in 'wind_v' with 0.2172\n",
      "Filled 16,836 missing values in 'evap_mm' with 2.9369\n",
      "Filled 20,273 missing values in 'pressure_hpa' with 978.2073\n",
      "Filled 16,836 missing values in 'soil_temp_c' with 25.7139\n",
      "Filled 720 missing values in 'ndvi' with 0.6749\n",
      "Filled 720 missing values in 'evi' with 0.4121\n",
      "Filled 117,325 missing values in 'lst' with 27.3900\n",
      "Filled 2,562 missing values in 'precip_mm' with 2.1642\n",
      "Filled 2,562 missing values in 'temp_c' with 25.0372\n",
      "Filled 2,562 missing values in 'dewpoint_c' with 20.9961\n",
      "Filled 2,562 missing values in 'wind_u' with -0.1192\n",
      "Filled 2,562 missing values in 'wind_v' with 0.2172\n",
      "Filled 2,562 missing values in 'evap_mm' with 2.9369\n",
      "Filled 3,581 missing values in 'pressure_hpa' with 978.2073\n",
      "Filled 2,562 missing values in 'soil_temp_c' with 25.7139\n",
      "Filled 112 missing values in 'ndvi' with 0.6749\n",
      "Filled 112 missing values in 'evi' with 0.4121\n",
      "Filled 115,928 missing values in 'lst' with 27.3900\n",
      "Filled 5,124 missing values in 'precip_mm' with 2.1642\n",
      "Filled 5,124 missing values in 'temp_c' with 25.0372\n",
      "Filled 5,124 missing values in 'dewpoint_c' with 20.9961\n",
      "Filled 5,124 missing values in 'wind_u' with -0.1192\n",
      "Filled 5,124 missing values in 'wind_v' with 0.2172\n",
      "Filled 5,124 missing values in 'evap_mm' with 2.9369\n",
      "Filled 5,929 missing values in 'pressure_hpa' with 978.2073\n",
      "Filled 5,124 missing values in 'soil_temp_c' with 25.7139\n",
      "Saved preprocessors to /Users/leonnn/Desktop/DL for disaster/models/preprocessors.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = DisasterDataProcessor(config.data)\n",
    "\n",
    "# Process data (load, handle missing values, normalize, split)\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_df, val_df, test_df = processor.process(DATA_PATH)\n",
    "\n",
    "# Save preprocessors for inference\n",
    "processor.save_preprocessors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d68906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights computed:\n",
      "  Flood: [0.5268532037734985, 9.809869766235352]\n",
      "  Drought: [0.5105904936790466, 24.106016159057617]\n",
      "\n",
      "üìä Data Distribution:\n",
      "   Training samples: 926,346\n",
      "   Validation samples: 198,738\n",
      "   Test samples: 198,738\n",
      "\n",
      "   Flood distribution (train):\n",
      "flood\n",
      "0    879131\n",
      "1     47215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Drought distribution (train):\n",
      "drought\n",
      "0    907132\n",
      "1     19214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weights(train_df)\n",
    "\n",
    "# Move weights to device\n",
    "flood_weights = class_weights['flood'].to(device)\n",
    "drought_weights = class_weights['drought'].to(device)\n",
    "\n",
    "print(f\"\\nüìä Data Distribution:\")\n",
    "print(f\"   Training samples: {len(train_df):,}\")\n",
    "print(f\"   Validation samples: {len(val_df):,}\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\n   Flood distribution (train):\")\n",
    "print(train_df['flood'].value_counts())\n",
    "print(f\"\\n   Drought distribution (train):\")\n",
    "print(train_df['drought'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55da7d",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d368d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n",
      "Created 911,160 valid samples\n",
      "Created 195,480 valid samples\n",
      "Created 195,480 valid samples\n",
      "\n",
      "‚úÖ DataLoaders created:\n",
      "   Train batches: 3,560\n",
      "   Val batches: 764\n",
      "   Test batches: 764\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_df, val_df, test_df, config.data\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "print(f\"   Train batches: {len(train_loader):,}\")\n",
    "print(f\"   Val batches: {len(val_loader):,}\")\n",
    "print(f\"   Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117a26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Sample batch shapes:\n",
      "   Satellite (CNN input): torch.Size([256, 3, 5, 5])\n",
      "   Weather (LSTM input): torch.Size([256, 7, 8])\n",
      "   Static (MLP input): torch.Size([256, 4])\n",
      "   Flood labels: torch.Size([256])\n",
      "   Drought labels: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Verify batch shapes\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"üì¶ Sample batch shapes:\")\n",
    "print(f\"   Satellite (CNN input): {sample_batch['satellite'].shape}\")\n",
    "print(f\"   Weather (LSTM input): {sample_batch['weather'].shape}\")\n",
    "print(f\"   Static (MLP input): {sample_batch['static'].shape}\")\n",
    "print(f\"   Flood labels: {sample_batch['flood_label'].shape}\")\n",
    "print(f\"   Drought labels: {sample_batch['drought_label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb278e3",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75192fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model Parameter Summary:\n",
      "==================================================\n",
      "  cnn_encoder: 126,720\n",
      "  lstm_encoder: 743,041\n",
      "  mlp_encoder: 25,536\n",
      "  fusion: 148,736\n",
      "  flood_head: 8,386\n",
      "  drought_head: 8,386\n",
      "  total: 1,060,805\n",
      "==================================================\n",
      "\n",
      "\n",
      "üèóÔ∏è Model Architecture:\n",
      "DisasterPredictionModel(\n",
      "  (cnn_encoder): CNNEncoder(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU(inplace=True)\n",
      "    )\n",
      "    (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm_encoder): LSTMEncoder(\n",
      "    (input_proj): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "    (attention): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp_encoder): MLPEncoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Dropout(p=0.3, inplace=False)\n",
      "      (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fusion): MidLevelFusion(\n",
      "    (fusion_mlp): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.4, inplace=False)\n",
      "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Dropout(p=0.4, inplace=False)\n",
      "      (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (flood_head): PredictionHead(\n",
      "    (head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (drought_head): PredictionHead(\n",
      "    (head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_model(config.model, device)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df89f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Forward pass successful!\n",
      "   Flood logits: torch.Size([256, 2])\n",
      "   Drought logits: torch.Size([256, 2])\n",
      "   CNN features: torch.Size([256, 128])\n",
      "   LSTM features: torch.Size([256, 128])\n",
      "   MLP features: torch.Size([256, 128])\n",
      "   Fused features: torch.Size([256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    satellite = sample_batch['satellite'].to(device)\n",
    "    weather = sample_batch['weather'].to(device)\n",
    "    static = sample_batch['static'].to(device)\n",
    "    \n",
    "    outputs = model(satellite, weather, static)\n",
    "    \n",
    "    print(\"\\n‚úÖ Forward pass successful!\")\n",
    "    print(f\"   Flood logits: {outputs['flood_logits'].shape}\")\n",
    "    print(f\"   Drought logits: {outputs['drought_logits'].shape}\")\n",
    "    print(f\"   CNN features: {outputs['cnn_features'].shape}\")\n",
    "    print(f\"   LSTM features: {outputs['lstm_features'].shape}\")\n",
    "    print(f\"   MLP features: {outputs['mlp_features'].shape}\")\n",
    "    print(f\"   Fused features: {outputs['fused_features'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82a6be",
   "metadata": {},
   "source": [
    "## 6. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e47122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Loss function with class weights\n",
    "criterion = MultiTaskLoss(\n",
    "    flood_weight=config.training.flood_loss_weight,\n",
    "    drought_weight=config.training.drought_loss_weight,\n",
    "    flood_class_weights=flood_weights,\n",
    "    drought_class_weights=drought_weights\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (verbose removed for PyTorch compatibility)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    patience=config.training.scheduler_patience,\n",
    "    factor=config.training.scheduler_factor,\n",
    "    min_lr=config.training.scheduler_min_lr\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.training.early_stopping_patience,\n",
    "    min_delta=config.training.early_stopping_delta,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Checkpoint manager\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    model_dir=MODEL_DIR,\n",
    "    experiment_name=config.experiment_name,\n",
    "    save_best_only=config.training.save_best_only,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = TrainingHistory()\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.training.use_amp and device.type == 'cuda' else None\n",
    "\n",
    "print(\"‚úÖ Training components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c4492",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7bb2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move data to device\n",
    "        satellite = batch['satellite'].to(device)\n",
    "        weather = batch['weather'].to(device)\n",
    "        static = batch['static'].to(device)\n",
    "        flood_labels = batch['flood_label'].to(device)\n",
    "        drought_labels = batch['drought_label'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if scaler:\n",
    "            with autocast():\n",
    "                outputs = model(satellite, weather, static)\n",
    "                losses = criterion(\n",
    "                    outputs['flood_logits'],\n",
    "                    outputs['drought_logits'],\n",
    "                    flood_labels,\n",
    "                    drought_labels\n",
    "                )\n",
    "            \n",
    "            # Backward pass with scaling\n",
    "            scaler.scale(losses['total_loss']).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip_value)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(satellite, weather, static)\n",
    "            losses = criterion(\n",
    "                outputs['flood_logits'],\n",
    "                outputs['drought_logits'],\n",
    "                flood_labels,\n",
    "                drought_labels\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            losses['total_loss'].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip_value)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        flood_preds = torch.argmax(outputs['flood_logits'], dim=1)\n",
    "        drought_preds = torch.argmax(outputs['drought_logits'], dim=1)\n",
    "        flood_probs = F.softmax(outputs['flood_logits'], dim=1)\n",
    "        drought_probs = F.softmax(outputs['drought_logits'], dim=1)\n",
    "        \n",
    "        metrics_calc.update('flood', flood_preds, flood_labels, flood_probs)\n",
    "        metrics_calc.update('drought', drought_preds, drought_labels, drought_probs)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f\"{losses['total_loss'].item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = metrics_calc.compute()\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move data to device\n",
    "        satellite = batch['satellite'].to(device)\n",
    "        weather = batch['weather'].to(device)\n",
    "        static = batch['static'].to(device)\n",
    "        flood_labels = batch['flood_label'].to(device)\n",
    "        drought_labels = batch['drought_label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(satellite, weather, static)\n",
    "        losses = criterion(\n",
    "            outputs['flood_logits'],\n",
    "            outputs['drought_logits'],\n",
    "            flood_labels,\n",
    "            drought_labels\n",
    "        )\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        flood_preds = torch.argmax(outputs['flood_logits'], dim=1)\n",
    "        drought_preds = torch.argmax(outputs['drought_logits'], dim=1)\n",
    "        flood_probs = F.softmax(outputs['flood_logits'], dim=1)\n",
    "        drought_probs = F.softmax(outputs['drought_logits'], dim=1)\n",
    "        \n",
    "        metrics_calc.update('flood', flood_preds, flood_labels, flood_probs)\n",
    "        metrics_calc.update('drought', drought_preds, drought_labels, drought_probs)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = metrics_calc.compute()\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95214ac0",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d8ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ STARTING TRAINING\n",
      "============================================================\n",
      "Epochs: 100\n",
      "Device: mps\n",
      "Mixed Precision: False\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c8b60aacd74ef18dc8892394b88ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {config.training.num_epochs}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Mixed Precision: {scaler is not None}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "best_val_f1 = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, config.training.num_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_metrics = validate_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Calculate average F1 for monitoring\n",
    "    val_f1_avg = (val_metrics.get('flood_f1', 0) + val_metrics.get('drought_f1', 0)) / 2\n",
    "    \n",
    "    # Update history\n",
    "    history.update(train_loss, val_loss, train_metrics, val_metrics, current_lr, epoch_time)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_f1_avg)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch}/{config.training.num_epochs} | Time: {format_time(epoch_time)}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Flood   - Train F1: {train_metrics.get('flood_f1', 0):.4f} | Val F1: {val_metrics.get('flood_f1', 0):.4f}\")\n",
    "    print(f\"  Drought - Train F1: {train_metrics.get('drought_f1', 0):.4f} | Val F1: {val_metrics.get('drought_f1', 0):.4f}\")\n",
    "    print(f\"  Average Val F1: {val_f1_avg:.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1_avg > best_val_f1:\n",
    "        best_val_f1 = val_f1_avg\n",
    "        checkpoint_manager.save(\n",
    "            model, optimizer, scheduler, epoch,\n",
    "            val_metrics, val_f1_avg\n",
    "        )\n",
    "        print(f\"  ‚úÖ New best model saved! F1: {val_f1_avg:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_f1_avg, epoch):\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {format_time(total_time)}\")\n",
    "print(f\"Best Val F1: {best_val_f1:.4f}\")\n",
    "print(f\"Best epoch: {early_stopping.best_epoch}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d4673",
   "metadata": {},
   "source": [
    "## 9. Save Training History and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = os.path.join(LOG_DIR, f\"{config.experiment_name}_history.json\")\n",
    "history.save(history_path)\n",
    "\n",
    "# Plot training curves\n",
    "fig = history.plot(save_path=os.path.join(LOG_DIR, f\"{config.experiment_name}_curves.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b3862",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_manager.load(model, filepath=os.path.join(MODEL_DIR, f\"{config.experiment_name}_best.pt\"))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_metrics = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nüåä Flood Prediction:\")\n",
    "print(f\"   Accuracy: {test_metrics.get('flood_accuracy', 0):.4f}\")\n",
    "print(f\"   Precision: {test_metrics.get('flood_precision', 0):.4f}\")\n",
    "print(f\"   Recall: {test_metrics.get('flood_recall', 0):.4f}\")\n",
    "print(f\"   F1 Score: {test_metrics.get('flood_f1', 0):.4f}\")\n",
    "if 'flood_roc_auc' in test_metrics:\n",
    "    print(f\"   ROC-AUC: {test_metrics.get('flood_roc_auc', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nüèúÔ∏è Drought Prediction:\")\n",
    "print(f\"   Accuracy: {test_metrics.get('drought_accuracy', 0):.4f}\")\n",
    "print(f\"   Precision: {test_metrics.get('drought_precision', 0):.4f}\")\n",
    "print(f\"   Recall: {test_metrics.get('drought_recall', 0):.4f}\")\n",
    "print(f\"   F1 Score: {test_metrics.get('drought_f1', 0):.4f}\")\n",
    "if 'drought_roc_auc' in test_metrics:\n",
    "    print(f\"   ROC-AUC: {test_metrics.get('drought_roc_auc', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Average Metrics:\")\n",
    "print(f\"   Accuracy: {test_metrics.get('avg_accuracy', 0):.4f}\")\n",
    "print(f\"   F1 Score: {test_metrics.get('avg_f1', 0):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed evaluation metrics and confusion matrices\n",
    "model.eval()\n",
    "all_flood_preds, all_flood_labels = [], []\n",
    "all_drought_preds, all_drought_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        satellite = batch['satellite'].to(device)\n",
    "        weather = batch['weather'].to(device)\n",
    "        static = batch['static'].to(device)\n",
    "        \n",
    "        outputs = model(satellite, weather, static)\n",
    "        \n",
    "        flood_preds = torch.argmax(outputs['flood_logits'], dim=1)\n",
    "        drought_preds = torch.argmax(outputs['drought_logits'], dim=1)\n",
    "        \n",
    "        all_flood_preds.extend(flood_preds.cpu().numpy())\n",
    "        all_flood_labels.extend(batch['flood_label'].numpy())\n",
    "        all_drought_preds.extend(drought_preds.cpu().numpy())\n",
    "        all_drought_labels.extend(batch['drought_label'].numpy())\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Flood confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_flood = confusion_matrix(all_flood_labels, all_flood_preds)\n",
    "sns.heatmap(cm_flood, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Flood', 'Flood'],\n",
    "            yticklabels=['No Flood', 'Flood'])\n",
    "axes[0].set_title('Flood Prediction Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Drought confusion matrix\n",
    "cm_drought = confusion_matrix(all_drought_labels, all_drought_preds)\n",
    "sns.heatmap(cm_drought, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['No Drought', 'Drought'],\n",
    "            yticklabels=['No Drought', 'Drought'])\n",
    "axes[1].set_title('Drought Prediction Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, f\"{config.experiment_name}_confusion_matrices.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba3161",
   "metadata": {},
   "source": [
    "## 11. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff80e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    \"experiment_name\": config.experiment_name,\n",
    "    \"model_config\": {\n",
    "        \"encoder_output_dim\": config.model.encoder_output_dim,\n",
    "        \"cnn_channels\": config.model.cnn_channels,\n",
    "        \"lstm_hidden_size\": config.model.lstm_hidden_size,\n",
    "        \"lstm_num_layers\": config.model.lstm_num_layers\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"num_epochs\": config.training.num_epochs,\n",
    "        \"learning_rate\": config.training.learning_rate,\n",
    "        \"batch_size\": config.data.batch_size,\n",
    "        \"sequence_length\": config.data.sequence_length\n",
    "    },\n",
    "    \"best_epoch\": early_stopping.best_epoch,\n",
    "    \"best_val_f1\": best_val_f1,\n",
    "    \"test_metrics\": {k: float(v) for k, v in test_metrics.items()},\n",
    "    \"total_training_time\": total_time\n",
    "}\n",
    "\n",
    "results_path = os.path.join(LOG_DIR, f\"{config.experiment_name}_results.json\")\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {results_path}\")\n",
    "print(f\"\\nüéâ Training complete! Model saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d5330",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook trained a multi-encoder deep learning model for natural disaster prediction:\n",
    "\n",
    "1. **CNN Encoder** processed satellite imagery features (NDVI, EVI, LST) to capture spatial patterns\n",
    "2. **LSTM Encoder** processed weather sequences to capture temporal patterns\n",
    "3. **MLP Encoder** processed static geographic features (elevation, landcover, coordinates)\n",
    "4. **Mid-Level Fusion** combined all encoder outputs\n",
    "5. **Dual Prediction Heads** made predictions for both flood and drought\n",
    "\n",
    "The model was trained with:\n",
    "- Class-weighted loss for handling imbalanced data\n",
    "- Learning rate scheduling\n",
    "- Early stopping\n",
    "- Mixed precision training (when available)\n",
    "\n",
    "Next steps:\n",
    "- Run the evaluation notebook for detailed analysis\n",
    "- Fine-tune hyperparameters if needed\n",
    "- Deploy model for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
