{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee6b09d",
   "metadata": {},
   "source": [
    "# üìä Natural Disaster Prediction - Model Evaluation\n",
    "\n",
    "## Big Data and Deep Learning-Based Natural Disaster Prediction Using Multi-Source Environmental Data\n",
    "\n",
    "This notebook provides comprehensive evaluation and visualization of the trained disaster prediction model.\n",
    "\n",
    "### Contents:\n",
    "1. Load trained model and test data\n",
    "2. Generate predictions\n",
    "3. Classification metrics (accuracy, precision, recall, F1)\n",
    "4. Confusion matrices\n",
    "5. ROC and Precision-Recall curves\n",
    "6. Feature importance analysis\n",
    "7. Error analysis\n",
    "8. Visualization of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651b1f3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b54999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840add8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from configs.config import (\n",
    "    get_config, DATA_PATH, MODEL_DIR, LOG_DIR,\n",
    "    SATELLITE_FEATURES, WEATHER_FEATURES, STATIC_FEATURES\n",
    ")\n",
    "from src.dataset import (\n",
    "    DisasterDataProcessor, create_dataloaders\n",
    ")\n",
    "from src.models import DisasterPredictionModel, create_model\n",
    "from src.utils import (\n",
    "    CheckpointManager, get_device, set_seed,\n",
    "    plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd407480",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = get_config()\n",
    "device = get_device()\n",
    "set_seed(config.data.random_seed)\n",
    "\n",
    "# Create model\n",
    "model = create_model(config.model, device)\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    model_dir=MODEL_DIR,\n",
    "    experiment_name=config.experiment_name\n",
    ")\n",
    "\n",
    "try:\n",
    "    checkpoint = checkpoint_manager.load(model)\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"   Best score: {checkpoint.get('best_score', 'N/A')}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è No checkpoint found. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "processor = DisasterDataProcessor(config.data)\n",
    "\n",
    "# Load preprocessors (fitted during training)\n",
    "try:\n",
    "    processor.load_preprocessors()\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Preprocessors not found. Running full preprocessing...\")\n",
    "\n",
    "# Process data\n",
    "train_df, val_df, test_df = processor.process(DATA_PATH)\n",
    "\n",
    "# Create test dataloader\n",
    "_, _, test_loader = create_dataloaders(train_df, val_df, test_df, config.data)\n",
    "\n",
    "print(f\"\\nüìä Test set: {len(test_df):,} samples\")\n",
    "print(f\"   Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380cfd6",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efda4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions\n",
    "model.eval()\n",
    "\n",
    "results = {\n",
    "    'flood_labels': [],\n",
    "    'flood_preds': [],\n",
    "    'flood_probs': [],\n",
    "    'drought_labels': [],\n",
    "    'drought_preds': [],\n",
    "    'drought_probs': [],\n",
    "    'grid_ids': [],\n",
    "    'dates': []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        # Move data to device\n",
    "        satellite = batch['satellite'].to(device)\n",
    "        weather = batch['weather'].to(device)\n",
    "        static = batch['static'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(satellite, weather, static)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        flood_probs = F.softmax(outputs['flood_logits'], dim=1)\n",
    "        drought_probs = F.softmax(outputs['drought_logits'], dim=1)\n",
    "        \n",
    "        flood_preds = torch.argmax(flood_probs, dim=1)\n",
    "        drought_preds = torch.argmax(drought_probs, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        results['flood_labels'].extend(batch['flood_label'].numpy())\n",
    "        results['flood_preds'].extend(flood_preds.cpu().numpy())\n",
    "        results['flood_probs'].extend(flood_probs[:, 1].cpu().numpy())  # Prob of positive class\n",
    "        \n",
    "        results['drought_labels'].extend(batch['drought_label'].numpy())\n",
    "        results['drought_preds'].extend(drought_preds.cpu().numpy())\n",
    "        results['drought_probs'].extend(drought_probs.cpu().numpy())\n",
    "        \n",
    "        results['grid_ids'].extend(batch['grid_id'])\n",
    "        results['dates'].extend(batch['date'])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for key in ['flood_labels', 'flood_preds', 'flood_probs', 'drought_labels', 'drought_preds']:\n",
    "    results[key] = np.array(results[key])\n",
    "\n",
    "results['drought_probs'] = np.array(results['drought_probs'])\n",
    "\n",
    "print(f\"\\n‚úÖ Generated predictions for {len(results['flood_labels']):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676bd27",
   "metadata": {},
   "source": [
    "## 4. Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, y_prob, task_name):\n",
    "    \"\"\"Print comprehensive metrics for a task\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {task_name.upper()} PREDICTION METRICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    print(f\"\\nüéØ Overall Metrics:\")\n",
    "    print(f\"   Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"   Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"   Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"   F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Positive class metrics (for disaster detection)\n",
    "    print(f\"\\nüö® Positive Class (Disaster) Metrics:\")\n",
    "    print(f\"   Precision: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
    "    print(f\"   Recall:    {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
    "    print(f\"   F1 Score:  {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
    "    \n",
    "    # ROC-AUC if binary\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        if y_prob.ndim > 1:\n",
    "            prob_positive = y_prob[:, 1] if y_prob.shape[1] > 1 else y_prob[:, 0]\n",
    "        else:\n",
    "            prob_positive = y_prob\n",
    "        print(f\"   ROC-AUC:   {roc_auc_score(y_true, prob_positive):.4f}\")\n",
    "        print(f\"   Avg Precision: {average_precision_score(y_true, prob_positive):.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['No ' + task_name, task_name]))\n",
    "\n",
    "# Flood metrics\n",
    "print_metrics(\n",
    "    results['flood_labels'],\n",
    "    results['flood_preds'],\n",
    "    results['flood_probs'],\n",
    "    'Flood'\n",
    ")\n",
    "\n",
    "# Drought metrics\n",
    "print_metrics(\n",
    "    results['drought_labels'],\n",
    "    results['drought_preds'],\n",
    "    results['drought_probs'],\n",
    "    'Drought'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e83ad",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03341ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Flood confusion matrix\n",
    "cm_flood = confusion_matrix(results['flood_labels'], results['flood_preds'])\n",
    "cm_flood_norm = cm_flood.astype('float') / cm_flood.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_flood, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Flood', 'Flood'],\n",
    "            yticklabels=['No Flood', 'Flood'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('üåä Flood Prediction\\nConfusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=10)\n",
    "axes[0].set_ylabel('True Label', fontsize=10)\n",
    "\n",
    "# Add percentages as secondary annotation\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j+0.5, i+0.7, f'({cm_flood_norm[i,j]*100:.1f}%)',\n",
    "                    ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "# Drought confusion matrix\n",
    "cm_drought = confusion_matrix(results['drought_labels'], results['drought_preds'])\n",
    "cm_drought_norm = cm_drought.astype('float') / cm_drought.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_drought, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['No Drought', 'Drought'],\n",
    "            yticklabels=['No Drought', 'Drought'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('üèúÔ∏è Drought Prediction\\nConfusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=10)\n",
    "axes[1].set_ylabel('True Label', fontsize=10)\n",
    "\n",
    "# Add percentages\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j+0.5, i+0.7, f'({cm_drought_norm[i,j]*100:.1f}%)',\n",
    "                    ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'confusion_matrices_detailed.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä Confusion Matrix Summary:\")\n",
    "print(f\"\\nüåä Flood:\")\n",
    "print(f\"   True Negatives (Correct No Flood): {cm_flood[0,0]:,}\")\n",
    "print(f\"   False Positives (False Alarm): {cm_flood[0,1]:,}\")\n",
    "print(f\"   False Negatives (Missed Flood): {cm_flood[1,0]:,}\")\n",
    "print(f\"   True Positives (Correct Flood): {cm_flood[1,1]:,}\")\n",
    "\n",
    "print(f\"\\nüèúÔ∏è Drought:\")\n",
    "print(f\"   True Negatives (Correct No Drought): {cm_drought[0,0]:,}\")\n",
    "print(f\"   False Positives (False Alarm): {cm_drought[0,1]:,}\")\n",
    "print(f\"   False Negatives (Missed Drought): {cm_drought[1,0]:,}\")\n",
    "print(f\"   True Positives (Correct Drought): {cm_drought[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de3910",
   "metadata": {},
   "source": [
    "## 6. ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# ===== FLOOD =====\n",
    "# ROC Curve - Flood\n",
    "fpr_flood, tpr_flood, _ = roc_curve(results['flood_labels'], results['flood_probs'])\n",
    "auc_flood = roc_auc_score(results['flood_labels'], results['flood_probs'])\n",
    "\n",
    "axes[0, 0].plot(fpr_flood, tpr_flood, color='blue', lw=2, \n",
    "                label=f'ROC curve (AUC = {auc_flood:.3f})')\n",
    "axes[0, 0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "axes[0, 0].set_xlim([0.0, 1.0])\n",
    "axes[0, 0].set_ylim([0.0, 1.05])\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('üåä Flood Prediction - ROC Curve', fontweight='bold')\n",
    "axes[0, 0].legend(loc=\"lower right\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - Flood\n",
    "precision_flood, recall_flood, _ = precision_recall_curve(results['flood_labels'], results['flood_probs'])\n",
    "ap_flood = average_precision_score(results['flood_labels'], results['flood_probs'])\n",
    "\n",
    "axes[0, 1].plot(recall_flood, precision_flood, color='blue', lw=2,\n",
    "                label=f'PR curve (AP = {ap_flood:.3f})')\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('üåä Flood Prediction - Precision-Recall Curve', fontweight='bold')\n",
    "axes[0, 1].legend(loc=\"lower left\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ===== DROUGHT =====\n",
    "drought_probs_positive = results['drought_probs'][:, 1] if results['drought_probs'].ndim > 1 else results['drought_probs']\n",
    "\n",
    "# ROC Curve - Drought\n",
    "fpr_drought, tpr_drought, _ = roc_curve(results['drought_labels'], drought_probs_positive)\n",
    "auc_drought = roc_auc_score(results['drought_labels'], drought_probs_positive)\n",
    "\n",
    "axes[1, 0].plot(fpr_drought, tpr_drought, color='orange', lw=2,\n",
    "                label=f'ROC curve (AUC = {auc_drought:.3f})')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('üèúÔ∏è Drought Prediction - ROC Curve', fontweight='bold')\n",
    "axes[1, 0].legend(loc=\"lower right\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - Drought\n",
    "precision_drought, recall_drought, _ = precision_recall_curve(results['drought_labels'], drought_probs_positive)\n",
    "ap_drought = average_precision_score(results['drought_labels'], drought_probs_positive)\n",
    "\n",
    "axes[1, 1].plot(recall_drought, precision_drought, color='orange', lw=2,\n",
    "                label=f'PR curve (AP = {ap_drought:.3f})')\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('üèúÔ∏è Drought Prediction - Precision-Recall Curve', fontweight='bold')\n",
    "axes[1, 1].legend(loc=\"lower left\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'roc_pr_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà AUC Scores:\")\n",
    "print(f\"   Flood ROC-AUC: {auc_flood:.4f}\")\n",
    "print(f\"   Flood Average Precision: {ap_flood:.4f}\")\n",
    "print(f\"   Drought ROC-AUC: {auc_drought:.4f}\")\n",
    "print(f\"   Drought Average Precision: {ap_drought:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ed75e",
   "metadata": {},
   "source": [
    "## 7. Probability Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a45bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Flood probability distribution\n",
    "axes[0].hist(results['flood_probs'][results['flood_labels'] == 0], bins=50, alpha=0.7, \n",
    "             label='No Flood (True)', color='green', density=True)\n",
    "axes[0].hist(results['flood_probs'][results['flood_labels'] == 1], bins=50, alpha=0.7,\n",
    "             label='Flood (True)', color='red', density=True)\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', label='Threshold (0.5)')\n",
    "axes[0].set_xlabel('Predicted Probability of Flood')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('üåä Flood Prediction Probability Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drought probability distribution\n",
    "axes[1].hist(drought_probs_positive[results['drought_labels'] == 0], bins=50, alpha=0.7,\n",
    "             label='No Drought (True)', color='green', density=True)\n",
    "axes[1].hist(drought_probs_positive[results['drought_labels'] == 1], bins=50, alpha=0.7,\n",
    "             label='Drought (True)', color='red', density=True)\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', label='Threshold (0.5)')\n",
    "axes[1].set_xlabel('Predicted Probability of Drought')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('üèúÔ∏è Drought Prediction Probability Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'probability_distributions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608d69d",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error analysis dataframe\n",
    "error_df = pd.DataFrame({\n",
    "    'grid_id': results['grid_ids'],\n",
    "    'date': results['dates'],\n",
    "    'flood_true': results['flood_labels'],\n",
    "    'flood_pred': results['flood_preds'],\n",
    "    'flood_prob': results['flood_probs'],\n",
    "    'drought_true': results['drought_labels'],\n",
    "    'drought_pred': results['drought_preds'],\n",
    "    'drought_prob': drought_probs_positive\n",
    "})\n",
    "\n",
    "# Add error flags\n",
    "error_df['flood_error'] = error_df['flood_true'] != error_df['flood_pred']\n",
    "error_df['drought_error'] = error_df['drought_true'] != error_df['drought_pred']\n",
    "error_df['flood_false_negative'] = (error_df['flood_true'] == 1) & (error_df['flood_pred'] == 0)\n",
    "error_df['flood_false_positive'] = (error_df['flood_true'] == 0) & (error_df['flood_pred'] == 1)\n",
    "error_df['drought_false_negative'] = (error_df['drought_true'] == 1) & (error_df['drought_pred'] == 0)\n",
    "error_df['drought_false_positive'] = (error_df['drought_true'] == 0) & (error_df['drought_pred'] == 1)\n",
    "\n",
    "print(\"üìä Error Analysis Summary:\")\n",
    "print(f\"\\nüåä Flood Errors:\")\n",
    "print(f\"   Total errors: {error_df['flood_error'].sum():,} ({error_df['flood_error'].mean()*100:.2f}%)\")\n",
    "print(f\"   False Negatives (Missed floods): {error_df['flood_false_negative'].sum():,}\")\n",
    "print(f\"   False Positives (False alarms): {error_df['flood_false_positive'].sum():,}\")\n",
    "\n",
    "print(f\"\\nüèúÔ∏è Drought Errors:\")\n",
    "print(f\"   Total errors: {error_df['drought_error'].sum():,} ({error_df['drought_error'].mean()*100:.2f}%)\")\n",
    "print(f\"   False Negatives (Missed droughts): {error_df['drought_false_negative'].sum():,}\")\n",
    "print(f\"   False Positives (False alarms): {error_df['drought_false_positive'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad15dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by confidence level\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Flood errors by confidence\n",
    "flood_errors = error_df[error_df['flood_error']]\n",
    "axes[0].hist(flood_errors['flood_prob'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Number of Errors')\n",
    "axes[0].set_title('üåä Flood Prediction Errors by Confidence', fontweight='bold')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drought errors by confidence\n",
    "drought_errors = error_df[error_df['drought_error']]\n",
    "axes[1].hist(drought_errors['drought_prob'], bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Number of Errors')\n",
    "axes[1].set_title('üèúÔ∏è Drought Prediction Errors by Confidence', fontweight='bold')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'error_confidence_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Errors near threshold (0.5) are borderline cases where model is uncertain.\")\n",
    "print(\"   High confidence errors indicate potential issues with certain patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc1d85",
   "metadata": {},
   "source": [
    "## 9. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d12f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_thresholds(y_true, y_prob, task_name):\n",
    "    \"\"\"Analyze different classification thresholds\"\"\"\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    metrics_by_threshold = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_prob >= thresh).astype(int)\n",
    "        metrics_by_threshold.append({\n",
    "            'threshold': thresh,\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'accuracy': accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_by_threshold)\n",
    "    \n",
    "    # Find optimal threshold for F1\n",
    "    best_idx = df['f1'].idxmax()\n",
    "    best_threshold = df.loc[best_idx, 'threshold']\n",
    "    best_f1 = df.loc[best_idx, 'f1']\n",
    "    \n",
    "    return df, best_threshold, best_f1\n",
    "\n",
    "# Analyze thresholds\n",
    "flood_thresh_df, flood_best_thresh, flood_best_f1 = analyze_thresholds(\n",
    "    results['flood_labels'], results['flood_probs'], 'Flood'\n",
    ")\n",
    "drought_thresh_df, drought_best_thresh, drought_best_f1 = analyze_thresholds(\n",
    "    results['drought_labels'], drought_probs_positive, 'Drought'\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Flood\n",
    "axes[0].plot(flood_thresh_df['threshold'], flood_thresh_df['precision'], label='Precision', marker='o')\n",
    "axes[0].plot(flood_thresh_df['threshold'], flood_thresh_df['recall'], label='Recall', marker='s')\n",
    "axes[0].plot(flood_thresh_df['threshold'], flood_thresh_df['f1'], label='F1', marker='^', linewidth=2)\n",
    "axes[0].axvline(x=flood_best_thresh, color='red', linestyle='--', label=f'Best F1 @ {flood_best_thresh:.2f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title(f'üåä Flood - Metrics vs Threshold\\nBest F1: {flood_best_f1:.4f} @ threshold {flood_best_thresh:.2f}', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drought\n",
    "axes[1].plot(drought_thresh_df['threshold'], drought_thresh_df['precision'], label='Precision', marker='o')\n",
    "axes[1].plot(drought_thresh_df['threshold'], drought_thresh_df['recall'], label='Recall', marker='s')\n",
    "axes[1].plot(drought_thresh_df['threshold'], drought_thresh_df['f1'], label='F1', marker='^', linewidth=2)\n",
    "axes[1].axvline(x=drought_best_thresh, color='red', linestyle='--', label=f'Best F1 @ {drought_best_thresh:.2f}')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title(f'üèúÔ∏è Drought - Metrics vs Threshold\\nBest F1: {drought_best_f1:.4f} @ threshold {drought_best_thresh:.2f}', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'threshold_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Optimal Thresholds:\")\n",
    "print(f\"   Flood: {flood_best_thresh:.2f} (F1: {flood_best_f1:.4f})\")\n",
    "print(f\"   Drought: {drought_best_thresh:.2f} (F1: {drought_best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b4e9d",
   "metadata": {},
   "source": [
    "## 10. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c38648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all evaluation results\n",
    "evaluation_results = {\n",
    "    \"evaluation_date\": datetime.now().isoformat(),\n",
    "    \"test_samples\": len(results['flood_labels']),\n",
    "    \"flood_metrics\": {\n",
    "        \"accuracy\": float(accuracy_score(results['flood_labels'], results['flood_preds'])),\n",
    "        \"precision\": float(precision_score(results['flood_labels'], results['flood_preds'])),\n",
    "        \"recall\": float(recall_score(results['flood_labels'], results['flood_preds'])),\n",
    "        \"f1\": float(f1_score(results['flood_labels'], results['flood_preds'])),\n",
    "        \"roc_auc\": float(auc_flood),\n",
    "        \"average_precision\": float(ap_flood),\n",
    "        \"optimal_threshold\": float(flood_best_thresh),\n",
    "        \"optimal_f1\": float(flood_best_f1),\n",
    "        \"confusion_matrix\": cm_flood.tolist()\n",
    "    },\n",
    "    \"drought_metrics\": {\n",
    "        \"accuracy\": float(accuracy_score(results['drought_labels'], results['drought_preds'])),\n",
    "        \"precision\": float(precision_score(results['drought_labels'], results['drought_preds'])),\n",
    "        \"recall\": float(recall_score(results['drought_labels'], results['drought_preds'])),\n",
    "        \"f1\": float(f1_score(results['drought_labels'], results['drought_preds'])),\n",
    "        \"roc_auc\": float(auc_drought),\n",
    "        \"average_precision\": float(ap_drought),\n",
    "        \"optimal_threshold\": float(drought_best_thresh),\n",
    "        \"optimal_f1\": float(drought_best_f1),\n",
    "        \"confusion_matrix\": cm_drought.tolist()\n",
    "    },\n",
    "    \"error_analysis\": {\n",
    "        \"flood_total_errors\": int(error_df['flood_error'].sum()),\n",
    "        \"flood_false_negatives\": int(error_df['flood_false_negative'].sum()),\n",
    "        \"flood_false_positives\": int(error_df['flood_false_positive'].sum()),\n",
    "        \"drought_total_errors\": int(error_df['drought_error'].sum()),\n",
    "        \"drought_false_negatives\": int(error_df['drought_false_negative'].sum()),\n",
    "        \"drought_false_positives\": int(error_df['drought_false_positive'].sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "eval_path = os.path.join(LOG_DIR, f\"{config.experiment_name}_evaluation.json\")\n",
    "with open(eval_path, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Evaluation results saved to {eval_path}\")\n",
    "\n",
    "# Save error dataframe for further analysis\n",
    "error_df.to_csv(os.path.join(LOG_DIR, 'error_analysis.csv'), index=False)\n",
    "print(f\"‚úÖ Error analysis saved to {os.path.join(LOG_DIR, 'error_analysis.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6bd54",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This evaluation notebook analyzed the disaster prediction model's performance:\n",
    "\n",
    "1. **Classification Metrics**: Accuracy, Precision, Recall, F1 for both tasks\n",
    "2. **Confusion Matrices**: Visualized true positives, false positives, etc.\n",
    "3. **ROC & PR Curves**: Analyzed discrimination ability and precision-recall tradeoff\n",
    "4. **Probability Distributions**: Examined model confidence for each class\n",
    "5. **Error Analysis**: Identified patterns in misclassifications\n",
    "6. **Threshold Optimization**: Found optimal classification thresholds\n",
    "\n",
    "### Key Insights:\n",
    "- Check the confusion matrices for class imbalance impact\n",
    "- ROC-AUC indicates overall discrimination ability\n",
    "- Optimal thresholds may differ from 0.5 for imbalanced classes\n",
    "- False negatives (missed disasters) are more critical than false positives"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
